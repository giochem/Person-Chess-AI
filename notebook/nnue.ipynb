{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNUE for Chess AI\n",
    "\n",
    "document: \n",
    "- https://www.kaggle.com/competitions/train-your-own-stockfish-nnue/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, we import the necessary libraries for data handling, model training, visualization, and UI creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import chess\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "- **Libraries**: We use PyTorch for the neural network, pandas for data loading, NumPy for array operations, scikit-learn for splitting data, python-chess for FEN parsing, TensorBoard for logging, and ipywidgets for the UI.\n",
    "- **Reproducibility**: Setting seeds ensures consistent results across runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parameter Configuration\n",
    "\n",
    "Here, we create an interactive UI using sliders to adjust key training parameters like `BATCH_SIZE` and `NUM_EPOCHS`. This allows you to change these values without modifying the code directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slider for batch size\n",
    "batch_size_slider = widgets.IntSlider(\n",
    "    value=512,    # Default value\n",
    "    min=64,       # Minimum value\n",
    "    max=1024,     # Maximum value\n",
    "    step=64,      # Step size\n",
    "    description='Batch Size:'\n",
    ")\n",
    "display(batch_size_slider)\n",
    "\n",
    "# Slider for number of epochs\n",
    "epochs_slider = widgets.IntSlider(\n",
    "    value=100,    # Default value\n",
    "    min=10,       # Minimum value\n",
    "    max=200,      # Maximum value\n",
    "    step=10,      # Step size\n",
    "    description='Epochs:'\n",
    ")\n",
    "display(epochs_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Use\n",
    "- Adjust the sliders to set your desired `BATCH_SIZE` (e.g., 64, 128, 256, etc.) and `NUM_EPOCHS` (e.g., 10, 50, 100, etc.).\n",
    "- The values are dynamically linked to the training process later in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading\n",
    "\n",
    "We load the chess dataset from a CSV file containing FEN strings and their evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (adjust the path as needed)\n",
    "df = pd.read_csv('../assets/chess-data/fen/train.csv')\n",
    "\n",
    "# Limit the dataset size for faster experimentation (optional)\n",
    "df = df[:100]  # Use first 1,024,000 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "- **CSV File**: Assumes a file with columns `FEN` (chess position) and `Evaluation` (numerical score).\n",
    "- **Limit**: Truncating the dataset to 1,024,000 rows speeds up processing; adjust or remove this based on your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "\n",
    "We convert FEN strings into a format suitable for the neural network by encoding chess boards as one-hot encoded arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the set of possible pieces (including empty square)\n",
    "pieces = list('rnbqkpRNBQKP.')  # 12 pieces + empty\n",
    "\n",
    "def one_hot_encode_piece(piece):\n",
    "    \"\"\"Convert a chess piece to a one-hot encoded vector.\"\"\"\n",
    "    arr = np.zeros(len(pieces), dtype=np.float32)\n",
    "    piece_to_index = {p: i for i, p in enumerate(pieces)}\n",
    "    arr[piece_to_index[piece]] = 1\n",
    "    return arr\n",
    "\n",
    "def encode_board(board):\n",
    "    \"\"\"Encode a chess board into a flat array of one-hot vectors.\"\"\"\n",
    "    board_str = str(board).replace(' ', '')  # Remove spaces\n",
    "    board_list = []\n",
    "    for row in board_str.split('\\n'):  # Split into rows\n",
    "        for piece in row:\n",
    "            board_list.append(one_hot_encode_piece(piece))\n",
    "    return np.array(board_list)  # Shape: (64, len(pieces))\n",
    "\n",
    "def encode_fen_string(fen_str):\n",
    "    \"\"\"Convert a FEN string to an encoded board.\"\"\"\n",
    "    board = chess.Board(fen=fen_str)\n",
    "    return encode_board(board)\n",
    "\n",
    "# Apply encoding to all FEN strings in the dataset\n",
    "df['FEN'] = df['FEN'].apply(encode_fen_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "- **One-Hot Encoding**: Each square on the 8x8 board is represented by a vector of length 13 (12 piece types + empty), resulting in a 64x13 input per position.\n",
    "- **Functions**: \n",
    "  - `one_hot_encode_piece`: Encodes a single piece.\n",
    "  - `encode_board`: Encodes the entire board.\n",
    "  - `encode_fen_string`: Parses FEN into a board and encodes it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Definition\n",
    "\n",
    "We define the NNUE model architecture using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNUE(nn.Module):\n",
    "    \"\"\"Efficiently Updatable Neural Network for chess evaluation.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(NNUE, self).__init__()\n",
    "        self.flatten = nn.Flatten()  # Flatten input from (64, 13) to (832,)\n",
    "        self.linear_tanh_stack = nn.Sequential(\n",
    "            nn.Linear(832, 256),  # Input: 64 squares * 13 piece types\n",
    "            nn.Tanh(),            # Activation\n",
    "            nn.Linear(256, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 8),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.output = nn.Linear(8, 1)  # Final output: single evaluation score\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)          # Flatten input tensor\n",
    "        x = self.linear_tanh_stack(x)  # Pass through hidden layers\n",
    "        return self.output(x)         # Output a single value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "- **Architecture**: A simple feedforward network with three hidden layers (256, 64, 8 neurons) and tanh activations, reducing the 832-dimensional input (64 * 13) to a single evaluation score.\n",
    "- **Purpose**: Predicts the evaluation of a chess position from its encoded representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Setup\n",
    "\n",
    "We prepare the data and set up the training components using the UI-defined parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract parameter values from sliders\n",
    "BATCH_SIZE = batch_size_slider.value\n",
    "NUM_EPOCHS = epochs_slider.value\n",
    "print(f'BATCH_SIZE: {BATCH_SIZE} \\tNUM_POCHS: {NUM_EPOCHS}')\n",
    "# Prepare input and target tensors\n",
    "X = np.stack(df['FEN'].values)  # Stack encoded FENs into a tensor\n",
    "y = df['Evaluation'].values     # Extract evaluations\n",
    "X = torch.tensor(X, dtype=torch.float32)  # Convert to PyTorch tensor\n",
    "y = torch.tensor(y, dtype=torch.float32)  # Convert to PyTorch tensor\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = NNUE()\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop\n",
    "\n",
    "We train the model and log the loss to TensorBoard for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter('runs/chess_nnue_experiment')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()        # Clear gradients\n",
    "        outputs = model(inputs)      # Forward pass\n",
    "        loss = criterion(outputs, targets.view(-1, 1))  # Compute loss\n",
    "        loss.backward()              # Backward pass\n",
    "        optimizer.step()             # Update weights\n",
    "        running_loss += loss.item()  # Accumulate loss\n",
    "    \n",
    "    # Calculate average loss for the epoch\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    writer.add_scalar('Loss/train', avg_loss, epoch + 1)  # Log to TensorBoard\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Close TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs --port 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation\n",
    "\n",
    "We evaluate the trained model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "\n",
    "# Disable gradient computation for evaluation\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs)      # Forward pass\n",
    "        loss = criterion(outputs, targets.view(-1, 1))  # Compute loss\n",
    "        test_loss += loss.item()     # Accumulate loss\n",
    "\n",
    "# Calculate and display average test loss\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
